import { Type, UnionMode } from '../constants.js';
import { invalidDataType } from '../data-types.js';
import { encodeIPC } from './encode-ipc.js';

/**
 * Encode an Arrow table into Arrow IPC binary format.
 * @param {import('../table.js').Table} table The Arrow table to encode.
 * @param {object} options Encoding options.
 * @param {import('./sink.js').Sink} [options.sink] IPC byte consumer.
 * @param {'stream' | 'file'} [options.format] Arrow stream or file format.
 * @returns {Uint8Array | null} The generated bytes (for an in-memory sink)
 *  or null (if using a sink that writes bytes elsewhere).
 */
export function tableToIPC(table, options) {
  // accept a format string option for Arrow-JS compatibility
  if (typeof options === 'string') {
    options = { format: options };
  }
  const schema = table.schema;
  const columns = table.children;
  const dictionaries = assembleDictionaryBatches(columns);
  const records = assembleRecordBatches(columns);
  const data = { schema, dictionaries, records };
  return encodeIPC(data, options).finish();
}

function assembleContext() {
  let byteLength = 0;
  const nodes = [];
  const regions = [];
  const buffers = [];
  const variadic = [];
  return {
    /**
     * @param {number} length
     * @param {number} nullCount
     */
    node(length, nullCount) {
      nodes.push({ length, nullCount });
    },
    /**
     * @param {import('../types.js').TypedArray} b
     */
    buffer(b) {
      const size = b.byteLength;
      const length = ((size + 7) & ~7);
      regions.push({ offset: byteLength, length });
      byteLength += length;
      buffers.push(new Uint8Array(b.buffer, b.byteOffset, size));
    },
    /**
     * @param {number} length
     */
    variadic(length) {
      variadic.push(length);
    },
    /**
     * @param {import('../types.js').DataType} type
     * @param {import('../batch.js').Batch} batch
     */
    children(type, batch) {
      // @ts-ignore
      type.children.forEach((field, index) => {
        visit(field.type, batch.children[index], this);
      });
    },
    /**
     * @returns {import('../types.js').RecordBatch}
     */
    done() {
      return { byteLength, nodes, regions, variadic, buffers };
    }
  };
}

/**
 * @param {import('../column.js').Column[]} columns
 * @returns {import('../types.js').DictionaryBatch[]}
 */
function assembleDictionaryBatches(columns) {
  const dictionaries = [];
  const seen = new Set;

  for (const col of columns) {
    const { type } = col;
    if (type.typeId !== -1) continue;
    if (seen.has(type.id)) continue;
    seen.add(type.id);

    // pass dictionary and deltas as-is
    // @ts-ignore
    const dict = col.data[0].dictionary;
    for (let i = 0; i < dict.data.length; ++i) {
      dictionaries.push({
        id: type.id,
        isDelta: i > 0,
        data: assembleRecordBatch([dict], i)
      });
    }
  }

  return dictionaries;
}

/**
 * @param {import('../column.js').Column[]} columns
 * @returns {import('../types.js').RecordBatch[]}
 */
function assembleRecordBatches(columns) {
  return (columns[0]?.data || [])
    .map((_, index) => assembleRecordBatch(columns, index));
}

/**
 * @param {import('../column.js').Column[]} columns
 * @returns {import('../types.js').RecordBatch}
 */
function assembleRecordBatch(columns, batchIndex = 0) {
  const ctx = assembleContext();
  columns.forEach(column => {
    visit(column.type, column.data[batchIndex], ctx);
  });
  return ctx.done();
}

/**
 * Visit a column batch, assembling buffer information.
 * @param {import('../types.js').DataType} type
 * @param {import('../batch.js').Batch} batch
 * @param {ReturnType<assembleContext>} ctx
 */
function visit(type, batch, ctx) {
  const { typeId } = type;

  // no field node, no buffers
  if (typeId === Type.Null) return;

  // record field node info
  ctx.node(batch.length, batch.nullCount);

  switch (typeId) {
    // validity and value buffers
    // backing dictionaries handled elsewhere
    case Type.Bool:
    case Type.Int:
    case Type.Time:
    case Type.Duration:
    case Type.Float:
    case Type.Date:
    case Type.Timestamp:
    case Type.Decimal:
    case Type.Interval:
    case Type.FixedSizeBinary:
    case Type.Dictionary: // dict key values
      ctx.buffer(batch.validity);
      ctx.buffer(batch.values);
      return;

    // validity, offset, and value buffers
    case Type.Utf8:
    case Type.LargeUtf8:
    case Type.Binary:
    case Type.LargeBinary:
      ctx.buffer(batch.validity);
      ctx.buffer(batch.offsets);
      ctx.buffer(batch.values);
      return;

    // views with variadic buffers
    case Type.BinaryView:
    case Type.Utf8View:
      ctx.buffer(batch.validity);
      ctx.buffer(batch.values);
      // @ts-ignore
      ctx.variadic(batch.data.length);
      // @ts-ignore
      batch.data.forEach(b => ctx.buffer(b));
      return;

    // validity, offset, and list child
    case Type.List:
    case Type.LargeList:
    case Type.Map:
      ctx.buffer(batch.validity);
      ctx.buffer(batch.offsets);
      ctx.children(type, batch);
      return;

    // validity, offset, size, and list child
    case Type.ListView:
    case Type.LargeListView:
      ctx.buffer(batch.validity);
      ctx.buffer(batch.offsets);
      ctx.buffer(batch.sizes);
      ctx.children(type, batch);
      return;

    // validity and children
    case Type.FixedSizeList:
    case Type.Struct:
      ctx.buffer(batch.validity);
      ctx.children(type, batch);
      return;

    // children only
    case Type.RunEndEncoded:
      ctx.children(type, batch);
      return;

    // union
    case Type.Union: {
      // @ts-ignore
      ctx.buffer(batch.typeIds);
      if (type.mode === UnionMode.Dense) {
        ctx.buffer(batch.offsets);
      }
      ctx.children(type, batch);
      return;
    }

    // unsupported type
    default:
      throw new Error(invalidDataType(typeId));
  }
}
